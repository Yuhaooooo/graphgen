# conda environments:
#
base                     /apps/anaconda3
DGCNN                    /home/FYP/heyu0012/.conda/envs/DGCNN
GCNN_GAP                 /home/FYP/heyu0012/.conda/envs/GCNN_GAP
graphgen              *  /home/FYP/heyu0012/.conda/envs/graphgen
pytorch                  /home/FYP/heyu0012/.conda/envs/pytorch

Using backend: pytorch
  0%|          | 0/189 [00:00<?, ?it/s] 59%|█████▊    | 111/189 [00:00<00:00, 1100.41it/s]100%|██████████| 189/189 [00:00<00:00, 1009.28it/s]Graphs counted 188
Successfully done node count 7
Successfully done edge count 1
{'node_forward': {'0': 0, '1': 1, '2': 2, '3': 3, '5': 4, '6': 5, '4': 6}, 'node_backward': {0: '0', 1: '1', 2: '2', 3: '3', 4: '5', 5: '6', 6: '4'}, 'edge_forward': {'0': 0}, 'edge_backward': {0: '0'}, 'max_nodes': 28, 'min_nodes': 10, 'max_edges': 33, 'min_edges': 10, 'max_degree': 4}
graphs_test: [[(6, 0), (8, 0), (16, 0), (17, 1), (19, 1), (24, 1), (33, 0), (43, 1), (48, 1), (52, 1), (55, 1), (66, 1), (72, 0), (77, 0), (78, 1), (79, 1), (82, 1), (84, 1), (90, 1), (93, 1), (102, 1), (110, 0), (115, 0), (120, 1), (122, 0), (124, 1), (127, 1), (133, 1), (142, 0), (145, 1), (146, 0), (154, 0), (157, 1), (162, 1), (169, 1), (170, 1), (182, 1), (184, 0)], [(1, 0), (3, 1), (15, 1), (18, 0), (21, 1), (27, 1), (30, 1), (37, 0), (39, 0), (45, 1), (47, 1), (50, 1), (54, 0), (57, 1), (58, 1), (62, 1), (68, 1), (81, 1), (85, 1), (86, 1), (92, 1), (94, 1), (97, 0), (99, 0), (104, 1), (106, 1), (116, 1), (118, 0), (119, 0), (129, 0), (130, 0), (134, 0), (137, 0), (158, 1), (165, 1), (166, 1), (173, 1), (176, 1)], [(4, 0), (10, 1), (11, 1), (13, 0), (20, 1), (22, 1), (28, 1), (29, 1), (34, 1), (44, 1), (51, 1), (53, 1), (61, 0), (69, 0), (75, 0), (76, 0), (87, 0), (88, 0), (91, 1), (96, 1), (98, 1), (100, 1), (101, 1), (103, 1), (109, 0), (111, 1), (125, 1), (139, 1), (143, 0), (147, 1), (149, 0), (156, 1), (164, 1), (171, 1), (172, 1), (181, 0), (183, 1), (185, 0)], [(0, 1), (2, 0), (9, 1), (23, 1), (26, 1), (35, 0), (38, 0), (40, 1), (41, 0), (59, 1), (60, 1), (63, 1), (67, 1), (83, 0), (95, 1), (107, 1), (114, 1), (117, 1), (123, 0), (131, 0), (135, 1), (138, 0), (144, 1), (148, 1), (152, 1), (153, 0), (155, 0), (160, 1), (161, 1), (163, 1), (168, 1), (175, 1), (177, 1), (178, 0), (179, 1), (186, 1), (187, 0)], [(5, 1), (7, 1), (12, 1), (14, 1), (25, 1), (31, 1), (32, 1), (36, 1), (42, 1), (46, 1), (49, 1), (56, 1), (64, 0), (65, 0), (70, 1), (71, 1), (73, 1), (74, 1), (80, 1), (89, 1), (105, 1), (108, 1), (112, 0), (113, 0), (121, 1), (126, 1), (128, 0), (132, 0), (136, 1), (140, 0), (141, 0), (150, 1), (151, 1), (159, 0), (167, 0), (174, 0), (180, 0)]]
Model: DFScodeRNN_cls
Device: cuda:0
Graph type: MUTAG
Training set: 150
Number of labels: 2
Max number of nodes: 28
Max number of edges: 33
Min number of nodes: 10
Min number of edges: 10
Max degree of a node: 4
No. of node labels: 7
No. of edge labels: 1

Epoch: 1/10000, train loss: 0.033877
Epoch: 2/10000, train loss: 0.035411
Epoch: 3/10000, train loss: 0.033615
Epoch: 4/10000, train loss: 0.034194
Epoch: 5/10000, train loss: 0.034072
Epoch: 6/10000, train loss: 0.034069
Epoch: 7/10000, train loss: 0.034807
Epoch: 8/10000, train loss: 0.034215
Epoch: 9/10000, train loss: 0.034377
Epoch: 10/10000, train loss: 0.033978
Epoch: 11/10000, train loss: 0.032979
Epoch: 12/10000, train loss: 0.033899
Epoch: 13/10000, train loss: 0.034095
Epoch: 14/10000, train loss: 0.033485
Epoch: 15/10000, train loss: 0.032340
Epoch: 16/10000, train loss: 0.034037
Epoch: 17/10000, train loss: 0.033715
Epoch: 18/10000, train loss: 0.032738
Epoch: 19/10000, train loss: 0.033334
Epoch: 20/10000, train loss: 0.033835
Model Saved - Epoch: 20/10000, train loss: 0.033835
Epoch: 21/10000, train loss: 0.032373
Epoch: 22/10000, train loss: 0.033906
Epoch: 23/10000, train loss: 0.033687
Epoch: 24/10000, train loss: 0.033247
Epoch: 25/10000, train loss: 0.034137
Epoch: 26/10000, train loss: 0.033177
Epoch: 27/10000, train loss: 0.033005
Epoch: 28/10000, train loss: 0.034329
Epoch: 29/10000, train loss: 0.033697
Epoch: 30/10000, train loss: 0.032837
Epoch: 31/10000, train loss: 0.034727
Epoch: 32/10000, train loss: 0.032245
Epoch: 33/10000, train loss: 0.033646
Epoch: 34/10000, train loss: 0.033726
Epoch: 35/10000, train loss: 0.035059
Epoch: 36/10000, train loss: 0.033340
Epoch: 37/10000, train loss: 0.033534
Epoch: 38/10000, train loss: 0.034186
Epoch: 39/10000, train loss: 0.034337
Epoch: 40/10000, train loss: 0.033020
Model Saved - Epoch: 40/10000, train loss: 0.033020
Epoch: 41/10000, train loss: 0.034111
Epoch: 42/10000, train loss: 0.033299
Epoch: 43/10000, train loss: 0.032779
Epoch: 44/10000, train loss: 0.034755
Epoch: 45/10000, train loss: 0.033767
Epoch: 46/10000, train loss: 0.033882
Epoch: 47/10000, train loss: 0.032817
Epoch: 48/10000, train loss: 0.032930
Epoch: 49/10000, train loss: 0.033064
Epoch: 50/10000, train loss: 0.033487
Epoch: 51/10000, train loss: 0.033185
Epoch: 52/10000, train loss: 0.033682
Epoch: 53/10000, train loss: 0.033501
Epoch: 54/10000, train loss: 0.033885
Epoch: 55/10000, train loss: 0.033255
Epoch: 56/10000, train loss: 0.033158
Epoch: 57/10000, train loss: 0.033707
Epoch: 58/10000, train loss: 0.032907
Epoch: 59/10000, train loss: 0.033413
Epoch: 60/10000, train loss: 0.033858
Model Saved - Epoch: 60/10000, train loss: 0.033858
Epoch: 61/10000, train loss: 0.034161
Epoch: 62/10000, train loss: 0.032333
Epoch: 63/10000, train loss: 0.033301
Epoch: 64/10000, train loss: 0.033945
Epoch: 65/10000, train loss: 0.032744
Epoch: 66/10000, train loss: 0.033742
Epoch: 67/10000, train loss: 0.033679
Epoch: 68/10000, train loss: 0.033308
Epoch: 69/10000, train loss: 0.033672
Epoch: 70/10000, train loss: 0.033136
Epoch: 71/10000, train loss: 0.033311
Epoch: 72/10000, train loss: 0.033303
Epoch: 73/10000, train loss: 0.033767
Epoch: 74/10000, train loss: 0.034146
Epoch: 75/10000, train loss: 0.032989
Epoch: 76/10000, train loss: 0.033360
Epoch: 77/10000, train loss: 0.034569
Epoch: 78/10000, train loss: 0.032857
Epoch: 79/10000, train loss: 0.033375
Epoch: 80/10000, train loss: 0.033008
Model Saved - Epoch: 80/10000, train loss: 0.033008
Epoch: 81/10000, train loss: 0.034400
Epoch: 82/10000, train loss: 0.034099
Epoch: 83/10000, train loss: 0.033607
Epoch: 84/10000, train loss: 0.033264
Epoch: 85/10000, train loss: 0.033539
Epoch: 86/10000, train loss: 0.032814
Epoch: 87/10000, train loss: 0.032519
Epoch: 88/10000, train loss: 0.033436
Epoch: 89/10000, train loss: 0.032967
Epoch: 90/10000, train loss: 0.033786
Epoch: 91/10000, train loss: 0.032770
Epoch: 92/10000, train loss: 0.032928
Epoch: 93/10000, train loss: 0.034061
Epoch: 94/10000, train loss: 0.032912
Epoch: 95/10000, train loss: 0.032450
Epoch: 96/10000, train loss: 0.033208
Epoch: 97/10000, train loss: 0.033798
Epoch: 98/10000, train loss: 0.034113
Epoch: 99/10000, train loss: 0.034287
Epoch: 100/10000, train loss: 0.033524
Model Saved - Epoch: 100/10000, train loss: 0.033524
Epoch: 101/10000, train loss: 0.034336
Epoch: 102/10000, train loss: 0.034405
Epoch: 103/10000, train loss: 0.033499
Epoch: 104/10000, train loss: 0.033468
Epoch: 105/10000, train loss: 0.033052
Epoch: 106/10000, train loss: 0.032495
Epoch: 107/10000, train loss: 0.034156
Epoch: 108/10000, train loss: 0.033055
Epoch: 109/10000, train loss: 0.033366
Epoch: 110/10000, train loss: 0.034648
Epoch: 111/10000, train loss: 0.034268
Epoch: 112/10000, train loss: 0.034329
Epoch: 113/10000, train loss: 0.033591
Epoch: 114/10000, train loss: 0.033556
Epoch: 115/10000, train loss: 0.034049
Epoch: 116/10000, train loss: 0.034061
Epoch: 117/10000, train loss: 0.033734
Epoch: 118/10000, train loss: 0.032750
Epoch: 119/10000, train loss: 0.034387
Epoch: 120/10000, train loss: 0.032690
Model Saved - Epoch: 120/10000, train loss: 0.032690
Epoch: 121/10000, train loss: 0.032630
Epoch: 122/10000, train loss: 0.033904
Epoch: 123/10000, train loss: 0.033608
Epoch: 124/10000, train loss: 0.032881
Epoch: 125/10000, train loss: 0.033038
Epoch: 126/10000, train loss: 0.032897
Epoch: 127/10000, train loss: 0.032374
Epoch: 128/10000, train loss: 0.033156
Epoch: 129/10000, train loss: 0.034445
Epoch: 130/10000, train loss: 0.033032
Epoch: 131/10000, train loss: 0.033718
Epoch: 132/10000, train loss: 0.033283
Epoch: 133/10000, train loss: 0.033171
Epoch: 134/10000, train loss: 0.033465
Epoch: 135/10000, train loss: 0.033773
Epoch: 136/10000, train loss: 0.033796
Epoch: 137/10000, train loss: 0.033285
Epoch: 138/10000, train loss: 0.033204
Epoch: 139/10000, train loss: 0.032749
Epoch: 140/10000, train loss: 0.034498
Model Saved - Epoch: 140/10000, train loss: 0.034498
Epoch: 141/10000, train loss: 0.033528
Epoch: 142/10000, train loss: 0.033271
Epoch: 143/10000, train loss: 0.033732
Epoch: 144/10000, train loss: 0.033811
Epoch: 145/10000, train loss: 0.033789
Epoch: 146/10000, train loss: 0.033787
Epoch: 147/10000, train loss: 0.032852
Epoch: 148/10000, train loss: 0.033416
Epoch: 149/10000, train loss: 0.033304
Epoch: 150/10000, train loss: 0.033943
Epoch: 151/10000, train loss: 0.034742
Epoch: 152/10000, train loss: 0.034272
Epoch: 153/10000, train loss: 0.033989
Epoch: 154/10000, train loss: 0.032970
Epoch: 155/10000, train loss: 0.033526
Epoch: 156/10000, train loss: 0.034279
Epoch: 157/10000, train loss: 0.034082
Epoch: 158/10000, train loss: 0.033206
Epoch: 159/10000, train loss: 0.033148
Epoch: 160/10000, train loss: 0.033570
Model Saved - Epoch: 160/10000, train loss: 0.033570
Epoch: 161/10000, train loss: 0.035010
Epoch: 162/10000, train loss: 0.033518
Epoch: 163/10000, train loss: 0.034088
Epoch: 164/10000, train loss: 0.034221
Epoch: 165/10000, train loss: 0.033326
Epoch: 166/10000, train loss: 0.034010
Epoch: 167/10000, train loss: 0.033682
Epoch: 168/10000, train loss: 0.033133
Epoch: 169/10000, train loss: 0.034195
Epoch: 170/10000, train loss: 0.033446
Epoch: 171/10000, train loss: 0.033385
Epoch: 172/10000, train loss: 0.032743
Epoch: 173/10000, train loss: 0.032899
Epoch: 174/10000, train loss: 0.033705
Epoch: 175/10000, train loss: 0.033297
Epoch: 176/10000, train loss: 0.034159
Epoch: 177/10000, train loss: 0.034120
Epoch: 178/10000, train loss: 0.033628
Epoch: 179/10000, train loss: 0.033308
Epoch: 180/10000, train loss: 0.034638
Model Saved - Epoch: 180/10000, train loss: 0.034638
Epoch: 181/10000, train loss: 0.032784
Epoch: 182/10000, train loss: 0.033820
Epoch: 183/10000, train loss: 0.033811
Epoch: 184/10000, train loss: 0.034708
Epoch: 185/10000, train loss: 0.033681
Epoch: 186/10000, train loss: 0.033686
Epoch: 187/10000, train loss: 0.033116
Epoch: 188/10000, train loss: 0.033619
Epoch: 189/10000, train loss: 0.033650
Epoch: 190/10000, train loss: 0.032955
Epoch: 191/10000, train loss: 0.033911
Epoch: 192/10000, train loss: 0.033793
Epoch: 193/10000, train loss: 0.033821
Epoch: 194/10000, train loss: 0.033355
Epoch: 195/10000, train loss: 0.033655
Epoch: 196/10000, train loss: 0.034290
Epoch: 197/10000, train loss: 0.034271
Epoch: 198/10000, train loss: 0.033490
Epoch: 199/10000, train loss: 0.033475
Epoch: 200/10000, train loss: 0.032994
Model Saved - Epoch: 200/10000, train loss: 0.032994
Epoch: 201/10000, train loss: 0.033789
Epoch: 202/10000, train loss: 0.033766
Epoch: 203/10000, train loss: 0.034016
Epoch: 204/10000, train loss: 0.033607
Epoch: 205/10000, train loss: 0.033903
Epoch: 206/10000, train loss: 0.032962
Epoch: 207/10000, train loss: 0.033112
Epoch: 208/10000, train loss: 0.032731
Epoch: 209/10000, train loss: 0.033365
Epoch: 210/10000, train loss: 0.033314
Epoch: 211/10000, train loss: 0.033859
Epoch: 212/10000, train loss: 0.033647
Epoch: 213/10000, train loss: 0.033667
Epoch: 214/10000, train loss: 0.034034
Epoch: 215/10000, train loss: 0.033543
Epoch: 216/10000, train loss: 0.033196
Epoch: 217/10000, train loss: 0.033901
Epoch: 218/10000, train loss: 0.033453
Epoch: 219/10000, train loss: 0.034503
Epoch: 220/10000, train loss: 0.032847
Model Saved - Epoch: 220/10000, train loss: 0.032847
Epoch: 221/10000, train loss: 0.032968
Epoch: 222/10000, train loss: 0.033687
Epoch: 223/10000, train loss: 0.032979
Epoch: 224/10000, train loss: 0.032596
Epoch: 225/10000, train loss: 0.034081
Epoch: 226/10000, train loss: 0.033906
Epoch: 227/10000, train loss: 0.033981
Epoch: 228/10000, train loss: 0.034057
Epoch: 229/10000, train loss: 0.033081
Epoch: 230/10000, train loss: 0.033287
Epoch: 231/10000, train loss: 0.033353
Epoch: 232/10000, train loss: 0.033402
Epoch: 233/10000, train loss: 0.033851
Epoch: 234/10000, train loss: 0.033745
Epoch: 235/10000, train loss: 0.033433
Epoch: 236/10000, train loss: 0.034021
Epoch: 237/10000, train loss: 0.034182
Epoch: 238/10000, train loss: 0.033667
Epoch: 239/10000, train loss: 0.033099
Epoch: 240/10000, train loss: 0.032939
Model Saved - Epoch: 240/10000, train loss: 0.032939
Epoch: 241/10000, train loss: 0.033176
Epoch: 242/10000, train loss: 0.033751
Epoch: 243/10000, train loss: 0.033884
Epoch: 244/10000, train loss: 0.032922
Epoch: 245/10000, train loss: 0.033961
Epoch: 246/10000, train loss: 0.032642
Epoch: 247/10000, train loss: 0.033055
Epoch: 248/10000, train loss: 0.033674
Epoch: 249/10000, train loss: 0.033054
Epoch: 250/10000, train loss: 0.033563
Epoch: 251/10000, train loss: 0.033330
Epoch: 252/10000, train loss: 0.033401
Epoch: 253/10000, train loss: 0.033080
Epoch: 254/10000, train loss: 0.032675
Epoch: 255/10000, train loss: 0.034619
Epoch: 256/10000, train loss: 0.032890
Epoch: 257/10000, train loss: 0.033841
Epoch: 258/10000, train loss: 0.032969
Epoch: 259/10000, train loss: 0.033019
Epoch: 260/10000, train loss: 0.033897
Model Saved - Epoch: 260/10000, train loss: 0.033897
Epoch: 261/10000, train loss: 0.033605
Epoch: 262/10000, train loss: 0.033268
Epoch: 263/10000, train loss: 0.033832
Epoch: 264/10000, train loss: 0.032877
Epoch: 265/10000, train loss: 0.033775
Epoch: 266/10000, train loss: 0.033221
Epoch: 267/10000, train loss: 0.032364
Epoch: 268/10000, train loss: 0.033457
Epoch: 269/10000, train loss: 0.033745
Epoch: 270/10000, train loss: 0.032434
Epoch: 271/10000, train loss: 0.034029
Epoch: 272/10000, train loss: 0.032952
Epoch: 273/10000, train loss: 0.034011
Epoch: 274/10000, train loss: 0.033413
Epoch: 275/10000, train loss: 0.033911
Epoch: 276/10000, train loss: 0.032655
Epoch: 277/10000, train loss: 0.032651
Epoch: 278/10000, train loss: 0.033714
Epoch: 279/10000, train loss: 0.033292
Epoch: 280/10000, train loss: 0.032935
Model Saved - Epoch: 280/10000, train loss: 0.032935
Epoch: 281/10000, train loss: 0.034169
Epoch: 282/10000, train loss: 0.032758
Epoch: 283/10000, train loss: 0.034270
Epoch: 284/10000, train loss: 0.033920
Epoch: 285/10000, train loss: 0.033826
Epoch: 286/10000, train loss: 0.033002
Epoch: 287/10000, train loss: 0.034008
Epoch: 288/10000, train loss: 0.033531
Epoch: 289/10000, train loss: 0.033847
Epoch: 290/10000, train loss: 0.033417
Epoch: 291/10000, train loss: 0.032577
Epoch: 292/10000, train loss: 0.034095
Epoch: 293/10000, train loss: 0.033646
Epoch: 294/10000, train loss: 0.034392
Epoch: 295/10000, train loss: 0.032450
Epoch: 296/10000, train loss: 0.034009
Epoch: 297/10000, train loss: 0.034433
Epoch: 298/10000, train loss: 0.032887
Epoch: 299/10000, train loss: 0.034159
Epoch: 300/10000, train loss: 0.033632
Model Saved - Epoch: 300/10000, train loss: 0.033632
Epoch: 301/10000, train loss: 0.033393
Epoch: 302/10000, train loss: 0.033405
Epoch: 303/10000, train loss: 0.033412
Epoch: 304/10000, train loss: 0.034629
Epoch: 305/10000, train loss: 0.033757
Epoch: 306/10000, train loss: 0.033555
Epoch: 307/10000, train loss: 0.033633
Epoch: 308/10000, train loss: 0.033322
Epoch: 309/10000, train loss: 0.033258
Epoch: 310/10000, train loss: 0.033295
Epoch: 311/10000, train loss: 0.033797
Epoch: 312/10000, train loss: 0.033126
Epoch: 313/10000, train loss: 0.033097
Epoch: 314/10000, train loss: 0.033462
Epoch: 315/10000, train loss: 0.033756
Epoch: 316/10000, train loss: 0.033654
Epoch: 317/10000, train loss: 0.033960
Epoch: 318/10000, train loss: 0.033152
Epoch: 319/10000, train loss: 0.033204
Epoch: 320/10000, train loss: 0.033570
Model Saved - Epoch: 320/10000, train loss: 0.033570
Epoch: 321/10000, train loss: 0.034036
Epoch: 322/10000, train loss: 0.033172
Epoch: 323/10000, train loss: 0.033561
Epoch: 324/10000, train loss: 0.032880
Epoch: 325/10000, train loss: 0.034066
Epoch: 326/10000, train loss: 0.033838
Epoch: 327/10000, train loss: 0.034023
Epoch: 328/10000, train loss: 0.032853
Epoch: 329/10000, train loss: 0.033315
Epoch: 330/10000, train loss: 0.034624
Epoch: 331/10000, train loss: 0.033467
Epoch: 332/10000, train loss: 0.034044
Epoch: 333/10000, train loss: 0.033883
Epoch: 334/10000, train loss: 0.034002
Epoch: 335/10000, train loss: 0.034030
Epoch: 336/10000, train loss: 0.033727
Epoch: 337/10000, train loss: 0.032952
Epoch: 338/10000, train loss: 0.032688
Epoch: 339/10000, train loss: 0.033988
Epoch: 340/10000, train loss: 0.032885
Model Saved - Epoch: 340/10000, train loss: 0.032885
Epoch: 341/10000, train loss: 0.034055
Epoch: 342/10000, train loss: 0.033509
Epoch: 343/10000, train loss: 0.034246
Epoch: 344/10000, train loss: 0.034478
Epoch: 345/10000, train loss: 0.033541
Epoch: 346/10000, train loss: 0.033525
Epoch: 347/10000, train loss: 0.033970
Epoch: 348/10000, train loss: 0.033622
Epoch: 349/10000, train loss: 0.032682
Epoch: 350/10000, train loss: 0.034257
Epoch: 351/10000, train loss: 0.032759
Epoch: 352/10000, train loss: 0.032835
Epoch: 353/10000, train loss: 0.033386
Epoch: 354/10000, train loss: 0.033460
Epoch: 355/10000, train loss: 0.034117
Epoch: 356/10000, train loss: 0.034669
Epoch: 357/10000, train loss: 0.032683
Epoch: 358/10000, train loss: 0.034194
Epoch: 359/10000, train loss: 0.033191
Epoch: 360/10000, train loss: 0.033045
Model Saved - Epoch: 360/10000, train loss: 0.033045
Epoch: 361/10000, train loss: 0.032690
Epoch: 362/10000, train loss: 0.033904
Epoch: 363/10000, train loss: 0.033393
Epoch: 364/10000, train loss: 0.033027
Epoch: 365/10000, train loss: 0.033447
Epoch: 366/10000, train loss: 0.033795
Epoch: 367/10000, train loss: 0.034035
Epoch: 368/10000, train loss: 0.033433
Epoch: 369/10000, train loss: 0.033649
Epoch: 370/10000, train loss: 0.033597
Epoch: 371/10000, train loss: 0.033047
Epoch: 372/10000, train loss: 0.033565
Epoch: 373/10000, train loss: 0.032944
Epoch: 374/10000, train loss: 0.033032
Epoch: 375/10000, train loss: 0.033739
Epoch: 376/10000, train loss: 0.033981
Epoch: 377/10000, train loss: 0.032710
Epoch: 378/10000, train loss: 0.033433
Epoch: 379/10000, train loss: 0.033161
Epoch: 380/10000, train loss: 0.033284
Model Saved - Epoch: 380/10000, train loss: 0.033284
Epoch: 381/10000, train loss: 0.034070
Epoch: 382/10000, train loss: 0.034211
Epoch: 383/10000, train loss: 0.034339
Epoch: 384/10000, train loss: 0.033708
Epoch: 385/10000, train loss: 0.033334
Epoch: 386/10000, train loss: 0.033386
Epoch: 387/10000, train loss: 0.033371
Epoch: 388/10000, train loss: 0.033178
Epoch: 389/10000, train loss: 0.033741
Epoch: 390/10000, train loss: 0.032584
Epoch: 391/10000, train loss: 0.033025
Epoch: 392/10000, train loss: 0.033224
Epoch: 393/10000, train loss: 0.033063
Epoch: 394/10000, train loss: 0.033275
Epoch: 395/10000, train loss: 0.034010
Epoch: 396/10000, train loss: 0.033465
Epoch: 397/10000, train loss: 0.033020
Epoch: 398/10000, train loss: 0.034180
Epoch: 399/10000, train loss: 0.032772
Epoch: 400/10000, train loss: 0.033382
Model Saved - Epoch: 400/10000, train loss: 0.033382
Epoch: 401/10000, train loss: 0.033557
Epoch: 402/10000, train loss: 0.033029
Epoch: 403/10000, train loss: 0.034133
Epoch: 404/10000, train loss: 0.033758
Epoch: 405/10000, train loss: 0.032957
Epoch: 406/10000, train loss: 0.033672
Epoch: 407/10000, train loss: 0.033613
Epoch: 408/10000, train loss: 0.033953
Epoch: 409/10000, train loss: 0.033876
Epoch: 410/10000, train loss: 0.032744
Epoch: 411/10000, train loss: 0.033567
Epoch: 412/10000, train loss: 0.033184
Epoch: 413/10000, train loss: 0.033826
Epoch: 414/10000, train loss: 0.034032
Epoch: 415/10000, train loss: 0.034161
Epoch: 416/10000, train loss: 0.033992
Epoch: 417/10000, train loss: 0.032998
Epoch: 418/10000, train loss: 0.033492
Epoch: 419/10000, train loss: 0.033391
Epoch: 420/10000, train loss: 0.034033
Model Saved - Epoch: 420/10000, train loss: 0.034033
Epoch: 421/10000, train loss: 0.033790
Epoch: 422/10000, train loss: 0.034701
Epoch: 423/10000, train loss: 0.032654
Epoch: 424/10000, train loss: 0.033565
Epoch: 425/10000, train loss: 0.034025
Epoch: 426/10000, train loss: 0.033465
Epoch: 427/10000, train loss: 0.032844
Epoch: 428/10000, train loss: 0.033107
Epoch: 429/10000, train loss: 0.033627
Epoch: 430/10000, train loss: 0.033877
Epoch: 431/10000, train loss: 0.033155
Epoch: 432/10000, train loss: 0.033283
Epoch: 433/10000, train loss: 0.032877
Epoch: 434/10000, train loss: 0.032925
Epoch: 435/10000, train loss: 0.033227
Epoch: 436/10000, train loss: 0.033748
Epoch: 437/10000, train loss: 0.033229
Epoch: 438/10000, train loss: 0.032908
Epoch: 439/10000, train loss: 0.033183
Epoch: 440/10000, train loss: 0.033918
Model Saved - Epoch: 440/10000, train loss: 0.033918
Epoch: 441/10000, train loss: 0.033391
Epoch: 442/10000, train loss: 0.033711
Epoch: 443/10000, train loss: 0.033292
Epoch: 444/10000, train loss: 0.033440
Epoch: 445/10000, train loss: 0.033332
Epoch: 446/10000, train loss: 0.033584
Epoch: 447/10000, train loss: 0.034574
Epoch: 448/10000, train loss: 0.034230
Epoch: 449/10000, train loss: 0.033654
Epoch: 450/10000, train loss: 0.033163
Epoch: 451/10000, train loss: 0.033699
Epoch: 452/10000, train loss: 0.033158
Epoch: 453/10000, train loss: 0.033555
Epoch: 454/10000, train loss: 0.033437
Epoch: 455/10000, train loss: 0.033567
Epoch: 456/10000, train loss: 0.033443
Epoch: 457/10000, train loss: 0.033450
Epoch: 458/10000, train loss: 0.033615
Epoch: 459/10000, train loss: 0.033746
Epoch: 460/10000, train loss: 0.032650
Model Saved - Epoch: 460/10000, train loss: 0.032650
Epoch: 461/10000, train loss: 0.033692
Epoch: 462/10000, train loss: 0.034001
Epoch: 463/10000, train loss: 0.033145
Epoch: 464/10000, train loss: 0.033153
Epoch: 465/10000, train loss: 0.033742
Epoch: 466/10000, train loss: 0.034254
Epoch: 467/10000, train loss: 0.033351
Epoch: 468/10000, train loss: 0.033088
Epoch: 469/10000, train loss: 0.033111
Epoch: 470/10000, train loss: 0.033175
Epoch: 471/10000, train loss: 0.033566
Epoch: 472/10000, train loss: 0.034255
Epoch: 473/10000, train loss: 0.033792
slurmstepd: error: *** JOB 11392 ON SCSEGPU-TC1-01 CANCELLED AT 2020-12-29T18:07:28 ***
